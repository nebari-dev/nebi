When I went ahead and pulled a tag of a particular workspace, modified it, and then pushed to a new tag, whenever I run Nebi workspace list local, it still shows up as the original tag with status modified rather than the new tag with status clean.  

`nebi ws info` it would be nice if it defaulted to the ws in the current dir if <ws> arg not passed in (look at .nebi file?)

I want Dash G to be a short flag for Dash Dash Global, for example on Nebi pull, but also others.

Add a flag to nebi pull, um, dash dash install, which will install the environment immediately after polling. And maybe a short flag dash i.

on `nebi ws list` the location gets cut off if too long.  There needs to at least be a way to see the whole path --long or something.  Maybe don't truncate it at all?
```
$ nebi ws list --local
WORKSPACE     TAG  STATUS  LOCATION
data-science  v1   clean   ~/eph/nebi/my-pixi-env (local)
data-science  v2   clean   ~/.local/share/nebi/... (global)
```

I think nebi shell needs a way to specify local in the case of a global and local pulls of same ws:tag, or interactive selection in the case of multiple local pulls of ws:tag.

future work: might be nice to override the PS1 look. e.g.
(data-science) [balast@nirvana my-pixi-env]$ becomes
(data-science:v1) [balast@nirvana my-pixi-env]$ 
but right now the nebi ws name != the pixi ws name so need to think through that a bit more.

remove `nebi workspace list tags` and add `nebi workspace tags` instead

on `nebi ws diff` why does it list changes in the pixi.toml and then when I add the --lock flag it says no changes.
```
(data-science) [balast@nirvana my-pixi-env]$ pixi add fastapi
âœ” Added fastapi >=0.128.0,<0.129

(data-science) [balast@nirvana my-pixi-env]$ nebi ws diff
--- pulled (data-science:v1, sha256:208ad206753a...)
+++ local
@@ pixi.toml @@
 [dependencies]
+fastapi = ">=0.128.0,<0.129"

@@ pixi.lock (changed) @@
[Use --lock for full lock file details]
(data-science) [balast@nirvana my-pixi-env]$ nebi ws diff --lock
--- pulled (data-science:v1, sha256:208ad206753a...)
+++ local
@@ pixi.toml @@
 [dependencies]
+fastapi = ">=0.128.0,<0.129"

  pixi.lock: no package changes
```

When creating a global environment I was allowed to create an alias for the global environment but it seemed like it wasn't used anywhere else like I'm not sure where it should be used but we should plan out where we would expect to be able to use that.

If your push fails, the workspace still shows up in nebi workspace list.  Not sure if that's desired or not, but we should consider it further.

If you pull when you already have the same versions of pixi.lock and pixi.toml locally, it'd be nice to skip the overwrite warning and just say no changes detected or something.  At least consider this.

when I run nebi pull blah-ds:v3 it says pulled blah-ds:v3 (version 1).  What is that version 1?  Is that per tag?  ignore this for now cuz might be consequence of other errors.  What defines a version?

I ran pixi add scipy.  then push blah-ds:v4, then pull blah-ds:v4 and scipy was gone.

How do you nebi diff two local versions?  Not saying we have to add it, but we should think about how to do it.

I see server_url in .nebi.  It's worth thinking about what happens if the server gets a new deployment (db erased).  What would happen in that case.  Also, if the server updates, do we have any backwards compatibility concerns.  I know some of this we might just have to think in the future about what's going on and what changes we're making, but like, anyway, is there any that we could foresee? It seems like there's, you know, changes to the REST API, obviously, you know, you'd need a new CLI version, but yeah maybe think through how to do server updates.

The nebi workspace name doesn't have to match the pixi workspace name.  Maybe we should add a column e.g. to `nebi ws list` commands to also list the pixi workspace names?  I think we that's confusing though so we call the nebi workspace repo instead and update all the cli commands and then we also list the pixi ws name.  But let's push the pixi ws renaming down the line.  Actually, maybe we keep the CLI command workspace, but in that output, and in the docs, like instead of calling the Nebi thing a workspace, we call it a repo. like in the cli docs it's `nebi push <repo>:<tag> for example.

We should write some docs giving a primer on the relevant portions of git, docker commands (push, pull, info, view manifest), OCI registries (manifest), etc. that we took inspiration from for the cli.

In the diff output, it seems like it might be more friendly to output
+ [feature.test.dependencies]
+ pytest = "*"

rather than 
 [feature]
+test = "map[dependencies:map[pytest:*]]"

as we do currently, but open for discussion if that's not easy for some reason or has other undesirable implications.

I kind of wish nebi serve was separated from teh rest of the available commans somehow b/c it's about starting a server.  Maybe brainstorm about how to do so and what would be best UX for users - consider displaying it in lower contrast in the cli if not too hard.  e.g. it's gray instead of white or black, but that could get tricky depending on the terminal bg color.

I think it's worth thinking through what happens if the .nebi file is deleted/lost unintentionally.  Like what no longer works.  What do each of the relevant cli commands change to.

[FUTURE] I think we should add a repair cli command to search for moved local workspaces by searching hardlink inode numbers assuming hardlinks are being used.  I assume that implies a filesystem that supports it, that the cache and the env are in the same drive mount (not sure if that's a requirement or not).  Not sure what to do if not using hardlinks, open to suggestions for how to handle that.

Need a cli command to list which server you are connected to.

---

I think nebi reg add should ideally verify the credentials at least a little bit when adding the registry.  I guess maybe this maybe there's like a lightweight like it can at least just issue a warning or something with that oh look the credentials don't exist.  Or maybe we have a flag to like bypass the check For that weird, you know, it seems like it should be at least check a little bit.  I know we mentioned like anonymous access or something to these repos, registries, so I don't know, maybe this will be hard to do in practice. I'm not sure.  Or maybe it's not practical because we'd have to define exactly what the permissions are supposed to be in the CLI to get the right error message?

I think we should have nebi push also work as nebi workspace push and nebi pull also work as nebi workspace pull (one just be a reference to the other).


[balast@nirvana my-pixi-env]$ nebi push deleteme:v1 
Note: This workspace was originally pulled from data-science:v1
  - pixi.lock modified locally
  - pixi.toml modified locally

Creating workspace "deleteme"...
Created workspace "deleteme"
Waiting for environment to be ready... done
Pushing deleteme:v1 to docker...
Error: Failed to push deleteme:v1: API error 500: {"error":"Failed to publish: failed to push to registry: failed to perform \"Exists\" on destination: HEAD \"https://registry-1.docker.io/v2/balast/deleteme/manifests/sha256:52602373da09b7842f24e484e98d2a50e4a815ede3fc808e257b50f23a441ca6\": GET \"https://auth.docker.io/token?scope=repository%3Abalast%2Fdeleteme%3Apull\u0026service=registry.docker.io\": response status code 401: Unauthorized"}
[balast@nirvana my-pixi-env]$ nebi ws list
NAME      STATUS  PACKAGE MANAGER  OWNER
deleteme  ready   pixi             admin
[balast@nirvana my-pixi-env]$ nebi workspace info
Local:
  Workspace: data-science:v1
  Server:    http://localhost:8460
  Pulled:    2026-01-23 06:16:15 (8 hours ago)
  Digest:    sha256:208ad206753abc98e947a415d3046873483970c1d2dcf6c6b5009da1fd93e7f3
  Status:    modified
    pixi.lock:   modified
    pixi.toml:   modified

Server:
  (workspace "data-science" not found on server)


The only thing we can push to Nebi is the pixi toml. We can't push the pixi lock file because Nebi needs to be able to enforce certain restrictions, potentially (license, etc.).  So whenever we, um, push, Nebi could potentially come up with a different lockfile, and maybe it should be, like, overwrite, or prompt to overwrite, or, you know, at least sometimes overwrite the lockfile right at the push, I guess it can't happen to me exactly right at the push, because at the Nebi server there needs to be a job picked up by a worker that then runs, generates the pixi lockfile, and only then could it push back locally.


--- DON'T SUBMIT TO LLM THINGS BELOW HERE ---
In the .darb file we have a server url, but should we have each server create a UUID (store in db) so even if the server is re-deployed at same location, the UUID gives it away as different.  Is there some other better method to handle this?  Low priority.

Should we allow pushing only to nebi server and not an OCI repo?  Local case, remote case?  Pull currently gets the info from nebi server db, but maybe we should just do more pass through to OCI repo by default.

Do we expect to be the only ones modifying the OCI repo?  For now prob yes, but eventually, at least need to handle the case where we aren't well, I'm guessing.

